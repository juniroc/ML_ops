# Chapter 3. Release Model

### 학습환경과 추론환경

- 학습환경과 추론환경은 다르다.
- 머신러닝 모델을 릴리즈하기 위해서는 **추론환경을 기준으로 모델을 제공**
- **추론 환경에 적합한 프로그램이나 라이브러리도 작성**

![Untitled](Chapter%203%20%20b3f2f/Untitled.png)

- 클라우드를 이용한 추론서버를 가동한다면, 컴퓨팅 자원의 스펙에 다라 시간 단위로 과금이 발생하기 때문에, **필요한 자원의 사용량을 최소한으로 조정하는 것이 이익으로 이어지는 구조**
    
    → 온프레미스 환경에서도 서버의 전력과 같은 운용 비용은 발생
    
- 당연히 추론환경에서는 학습환경과 다르게 `.ipynb` 가 아닌 `.py` 파일로 파이썬 모듈을 실행

![Untitled](Chapter%203%20%20b3f2f/Untitled%201.png)

- ONNX Runtime, Tensorflow Serving(TFServe), BentoML 등 다양한 추론용 라이브러리 존재

- 학습환경과 추론환경은 서로 전혀 다른 시스템 구성이 필요하지만, 공통으로 사용하는 컴포넌트 존재
→ 전처리 등
- **실제 도입한 시스템에서 프로그램이 정상 작동하는 반면, 데이터의 타입이 다르다는 이유로 예상 밖의 추론 결과가 출력되는 장애를 겪는 사례도 존재**

---

### 안티패턴 (버저닝 문제)

- 학습환경과 추론환경에서 같은 라이브러리를 사용하나, 라이브러리 버전이 일치하지 않는 경우
→ 추론기로 모델을 불러올 수 없는 경우
→ 추론기의 추론 결과가 학습환경에서 예상했던 추론 결과와 일치하지 않는 경우

- 버전 호환성 문제를 회피하기 위해 **ONNX 를 사용하는 방법 존재
→ 서로 다른 라이브러리**에서 학습한 모델을 **공통된 ONNX 형식으로 변환**해주는 역할
→ pickle 형식으로 저장하지 않아도 됨
→ 버전에 따른 문제가 일어나지 않음.
    - 그러나 모든 연산을 지원하는 것은 아님
    (21년 02월 기준, 커스터마이즈한 transformer 는 지원 안함)

- 또는 버전을 포함한 라이브러리 목록만 가지고 있으면 라이브러리 전체를 설치할 수 있음
`pip install -r requirements.txt` 를 이용
→ 이때 `pip list freeze > requirements.txt` 를 통해 `requirements.txt` 생성 가능

---

### 모델 배포 및 추론기 가동

- 모델을 pkl, th, SavedModel, onnx 같은 파일로 저장할 수 있고, 이것을 추론기에 담는 방법도 하나의 시스템으로 설계할 필요있음
→ 이때 겪는 어려움
    1. **모델 파일이 수MB 이상인 경우 (무거운 경우)**
    2. **배포 대상 추론기와의 호환성**
    3. **인벤토리 관리**

1. **모델 파일이 수MB 이상인 경우 (무거운 경우)**
- 모델 배포하고 교체하는 과정에서 로딩만 수십 초 정도가 소요될 수 있음
→ 실제 시스템이 가동 중인 경우, 모델 배포하고 갱신하는 중에 시스템이 멈추지 않도록 검토해야함
ex) **카나리 릴리스 방식 등** 이용하면 해결 가능하다

1. **배포 대상 추론기와의 호환성**
- 역시나 라이브러리 버저닝 문제
- 버젼과 모델이 일치해서 정상적인 추론 결과가 도출

1. **인벤토리 관리**
- 가동중인 추론기의 OS, library, version, model, data type/format, purpose of model(classification, regression)을 일원화하여 관리
- 세월이 흘러 담당 엔지니어가 바뀌는 경우 알 수 없는 추론기만 남겨지는 사태 발생..

### 학습 및 추론 환경의 라이브러리와 버전 선정

- 지나치게 학습 모델에 치우칠 필요없으나, 취약점이 있는 버전의 라이브러리를 사용하는 행위는 피함.
- 실제 학습과 추론에서 동일한 라이브러리를 사용할 필요가 있을 때,
ex) scikit-learn, OpenCV, MeCab 등
추론기 개발하는 팀이 엄격하게 라이브러리를 선정하는 경우가 많음
→ 릴리즈 조건으로 **보안 기준이나 지연과 같은 비기능요건이 포함**되는 경우가 있기 때문

### 라이브러리 취약성 관련 정보 DB

- CVE ( **[https://cve.mitre.org/index.html](https://cve.mitre.org/index.html)** )
- JVN ( [https://jvndb.jvn.jp/](https://jvndb.jvn.jp/) )

### 추론기에 모델 포함하기

- 필요 컴포넌트
    - 인프라 : 서버, CPU, 메모리, 스토리지, 네트워크
    - OS : Linux, Windows 등
    - 런타임 : 머신러닝 모델을 불러오고 가동하기 위한 라이브러리. 추론 전용 라이브러리에서는 ONNX Runtime or TensorFlow Serving 등, 학습-추론 공용일 경우 scikit-learn 등.
    - 모델 파일 : 이미 학습된 모델 파일
    - 프로그램 : 추론 요청에 대해 전처리, 추론, 후처리를 수행하고 응답하는 프로그램

![Untitled](Chapter%203%20%20b3f2f/Untitled%202.png)

---

### 모델-인-이미지 패턴

- **추론기의 이미지에 모델 파일을 포함해 빌드** 하는 패턴
    - 서버 이미지와 추론 모델의 버전을 일치시키고 싶은 경우
    - 추론 모델에 개별 서버 이미지를 준비하는 경우

- 릴리즈 전에 서버로 모델을 불러와 추론이 가능한 상태로 만들어야 함.
**→ 모델을 포함한 서버를 빌드함
→ 즉, 서버와 모델의 버전을 일치시킬 수 있어, 정상적인 가동이 가능한 서버를 모델과 일대일로 정리할 수 있다는 장점**

![Untitled](Chapter%203%20%20b3f2f/Untitled%203.png)

- 학습을 통해 모델 생성 후 모델을 포함한 추론용 서버 이미지를 빌드 후, 이미지를 풀(Pull) 하여 기동시킴.
- 서버 이미지를 빌드하는 소요시간이 길고, 용량이 증가한다는 단점 존재
→ 서버 이미지의 용량이 증가함에 따라 이미지를 풀(pull)하고 시스템이 가동될 때까지 소요 시간이 길어짐.
- 학습이 완료된 이후에 이뤄지기에 전 과정을 아울러 구축을 완료하는 파이프라인 필요.
- 이때, 빌드를 하더라도 원래의 모델 파일을 서버 이미지와 별개로 저장해 두는 것 권장됨 (백업용도)

---

### 구현

- 추론기의 인프라로서 `k8s 클러스터`로 알려진 **컨테이너 실행 기반 사용**
- 웹 API 로 가동시켜 GET/POST 요청으로 접근 가능케 함
    -